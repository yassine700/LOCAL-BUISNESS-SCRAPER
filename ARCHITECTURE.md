# System Architecture - Generic Proxy API

## High-Level Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          USER INTERFACE (Frontend)                          â•‘
â•‘                                                                            â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ Business Scraper                                                     â”‚ â•‘
â•‘  â”‚                                                                      â”‚ â•‘
â•‘  â”‚ Search Keyword: [___________]  "restaurant"                         â”‚ â•‘
â•‘  â”‚ Cities:        [___________]  Toledo, OH                            â”‚ â•‘
â•‘  â”‚                [___________]  St Pete, FL                           â”‚ â•‘
â•‘  â”‚                [___________]  ...                                   â”‚ â•‘
â•‘  â”‚                                                                      â”‚ â•‘
â•‘  â”‚ Proxy API Key (Optional):                                           â”‚ â•‘
â•‘  â”‚ [________________]  (password field)                                â”‚ â•‘
â•‘  â”‚                                                                      â”‚ â•‘
â•‘  â”‚ ğŸ“¡ Direct scraping (FREE): Leave empty                             â”‚ â•‘
â•‘  â”‚ ğŸ”‘ With proxy: Add ScrapingBee, Bright Data, Apify, etc.           â”‚ â•‘
â•‘  â”‚                                                                      â”‚ â•‘
â•‘  â”‚                        [â–¶ Start Scraping]                           â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                            â•‘
â•‘  WebSocket Connection: ws://localhost:8000/ws/{job_id}                    â•‘
â•‘  Real-time Updates: Status, Progress, Results                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â”‚ POST /api/scrape
                                    â”‚ {keyword, cities, sources, proxy_api_key}
                                    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     BACKEND API (FastAPI, Port 8000)                       â•‘
â•‘                                                                            â•‘
â•‘  POST /api/scrape                                                         â•‘
â•‘  â”œâ”€ Accept: keyword, cities, sources, proxy_api_key (optional)           â•‘
â•‘  â”œâ”€ Validate inputs                                                       â•‘
â•‘  â”œâ”€ Generate job_id (UUID)                                                â•‘
â•‘  â”œâ”€ Store proxy_api_key if provided                                       â•‘
â•‘  â””â”€ Spawn Celery tasks (1 per city)                                      â•‘
â•‘                                                                            â•‘
â•‘  GET /api/status/{job_id}                                                 â•‘
â•‘  â””â”€ Return: job status, progress, business count                         â•‘
â•‘                                                                            â•‘
â•‘  WebSocket /ws/{job_id}                                                   â•‘
â•‘  â””â”€ Send real-time updates from Redis pub/sub                            â•‘
â•‘                                                                            â•‘
â•‘  GET /api/download/{job_id}                                              â•‘
â•‘  â””â”€ Return: CSV file with results                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â”‚ Celery task dispatch
                                    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              BACKGROUND TASK QUEUE (Celery + Redis Broker)                 â•‘
â•‘                                                                            â•‘
â•‘  create_scraping_job_task()                                               â•‘
â•‘  â”œâ”€ Create job record in database                                         â•‘
â•‘  â”œâ”€ Store proxy_api_key in PROXY_API_KEY env var                         â•‘
â•‘  â””â”€ For each city:                                                        â•‘
â•‘      â””â”€ Dispatch scrape_business_task(job_id, keyword, city)             â•‘
â•‘                                                                            â•‘
â•‘  scrape_business_task(job_id, keyword, city)                             â•‘
â•‘  â””â”€ Invoke YellowPagesScraper.scrape()                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   SCRAPER ENGINE (YellowPagesScraper)                      â•‘
â•‘                                                                            â•‘
â•‘  For each page in YellowPages search:                                     â•‘
â•‘  â”‚                                                                        â•‘
â•‘  â”œâ”€ Option A: Direct IP Scraping (No API key)                           â•‘
â•‘  â”‚  â”œâ”€ Build: https://www.yellowpages.com/search?...                   â•‘
â•‘  â”‚  â”œâ”€ Fetch with httpx (async HTTP client)                            â•‘
â•‘  â”‚  â”œâ”€ Parse HTML with BeautifulSoup                                   â•‘
â•‘  â”‚  â””â”€ Extract listings with lxml                                       â•‘
â•‘  â”‚                                                                        â•‘
â•‘  â””â”€ Option B: Proxy API Scraping (With API key)                          â•‘
â•‘     â”œâ”€ Check: USE_PROXY = bool(PROXY_API_KEY)                           â•‘
â•‘     â”œâ”€ Initialize: ProxyAPIClient()                                      â•‘
â•‘     â”‚  â”œâ”€ Read API key: os.getenv("PROXY_API_KEY")                      â•‘
â•‘     â”‚  â””â”€ Support: ScrapingBee, Bright Data, Apify, etc.               â•‘
â•‘     â”‚                                                                     â•‘
â•‘     â””â”€ Fetch page through proxy service:                                 â•‘
â•‘        â”œâ”€ POST to service API with URL                                   â•‘
â•‘        â”œâ”€ Receive rendered HTML                                          â•‘
â•‘        â”œâ”€ Parse with BeautifulSoup                                       â•‘
â•‘        â””â”€ Extract listings                                               â•‘
â•‘                                                                            â•‘
â•‘  For each listing on page:                                               â•‘
â•‘  â”œâ”€ Extract basic info (name, phone, category)                          â•‘
â•‘  â”œâ”€ Visit detail page (same A/B scraping method)                        â•‘
â•‘  â””â”€ Extract website using:                                               â•‘
â•‘     â”œâ”€ JSON-LD structured data (priority)                               â•‘
â•‘     â”œâ”€ Heuristic patterns (fallback)                                    â•‘
â•‘     â””â”€ Regex patterns (final fallback)                                  â•‘
â•‘                                                                            â•‘
â•‘  For each business extracted:                                            â•‘
â•‘  â”œâ”€ Check BLOCKED_DOMAINS filter                                         â•‘
â•‘  â”œâ”€ Save to SQLite (deduplication)                                       â•‘
â•‘  â”œâ”€ Emit event to Redis pub/sub                                          â•‘
â•‘  â””â”€ Update job progress                                                  â•‘
â•‘                                                                            â•‘
â•‘  After all pages:                                                        â•‘
â•‘  â”œâ”€ Calculate extraction statistics                                      â•‘
â•‘  â”œâ”€ Emit metrics to Redis                                                â•‘
â•‘  â””â”€ Mark task complete                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              PROXY API CLIENT (Generic - Works with Any Service)           â•‘
â•‘                                                                            â•‘
â•‘  Class: ProxyAPIClient                                                    â•‘
â•‘  â”œâ”€ Supports: ScrapingBee, Bright Data, Apify, etc.                     â•‘
â•‘  â”‚                                                                        â•‘
â•‘  â”œâ”€ Initialize:                                                          â•‘
â•‘  â”‚  â””â”€ Read API key from PROXY_API_KEY environment variable             â•‘
â•‘  â”‚                                                                        â•‘
â•‘  â”œâ”€ Method: fetch_url(url, country_code="us", premium_proxy=True)      â•‘
â•‘  â”‚  â”œâ”€ Build request parameters                                         â•‘
â•‘  â”‚  â”œâ”€ POST to proxy API (currently: ScrapingBee, can change)          â•‘
â•‘  â”‚  â”œâ”€ Retry on rate limit or server error                            â•‘
â•‘  â”‚  â””â”€ Return HTML response                                             â•‘
â•‘  â”‚                                                                        â•‘
â•‘  â””â”€ Error Handling:                                                      â•‘
â•‘     â”œâ”€ 400: Invalid API key                                             â•‘
â•‘     â”œâ”€ 429: Rate limited (retry with backoff)                          â•‘
â•‘     â”œâ”€ 500+: Server error (retry)                                       â•‘
â•‘     â””â”€ Timeout: Network error (retry)                                   â•‘
â•‘                                                                            â•‘
â•‘  Backward Compatibility:                                                 â•‘
â•‘  â”œâ”€ Alias: ScrapingBeeClient = ProxyAPIClient                           â•‘
â•‘  â””â”€ Function: get_scrapingbee_client() â†’ ProxyAPIClient()              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        DATABASE (SQLite)                                   â•‘
â•‘                                                                            â•‘
â•‘  Tables:                                                                 â•‘
â•‘  â”œâ”€ jobs (job_id, keyword, status, started_at, completed_at)           â•‘
â•‘  â”œâ”€ tasks (job_id, keyword, city, status, progress)                    â•‘
â•‘  â””â”€ businesses (                                                        â•‘
â•‘      job_id, business_name, website, city, source,                    â•‘
â•‘      UNIQUE(business_name, website, city, source)  â† Deduplication    â•‘
â•‘     )                                                                    â•‘
â•‘                                                                            â•‘
â•‘  Purpose:                                                                â•‘
â•‘  â”œâ”€ Store results persistently                                          â•‘
â•‘  â”œâ”€ Track job progress                                                  â•‘
â•‘  â”œâ”€ Enable pause/resume                                                â•‘
â•‘  â””â”€ Support pagination on resume                                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                    â”‚
                                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚                                     â”‚
                                    â†“                                     â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 REDIS MESSAGE QUEUE (Real-time Events)                     â•‘
â•‘                                                                            â•‘
â•‘  Channels:                                                               â•‘
â•‘  â”œâ”€ job:{job_id}:events                                                 â•‘
â•‘  â”‚  â””â”€ Business scraped events (for WebSocket broadcasting)             â•‘
â•‘  â”‚                                                                       â•‘
â•‘  â””â”€ job:{job_id}:metrics                                                â•‘
â•‘     â””â”€ Extraction statistics (methods used, success rates)              â•‘
â•‘                                                                           â•‘
â•‘  Message Format:                                                        â•‘
â•‘  {                                                                       â•‘
â•‘    "type": "business" | "extraction_stats",                            â•‘
â•‘    "job_id": "uuid",                                                    â•‘
â•‘    "data": {...}                                                        â•‘
â•‘  }                                                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                                                             â”‚
                                                                             â””â”€ Broadcast to connected WebSocket clients
                                                                                    â”‚
                                                                                    â†“
                                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                                    â”‚ Frontend WebSocket Updateâ”‚
                                                                    â”‚ - Refresh business count â”‚
                                                                    â”‚ - Update progress bar    â”‚
                                                                    â”‚ - Add row to results     â”‚
                                                                    â”‚ - Show current city      â”‚
                                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration Flow

```
User submits form with optional proxy_api_key
    â”‚
    â†“
Frontend sends POST /api/scrape {proxy_api_key: "key"}
    â”‚
    â†“
Backend stores in environment: os.environ["PROXY_API_KEY"] = proxy_api_key
    â”‚
    â†“
Celery worker reads: PROXY_API_KEY = os.getenv("PROXY_API_KEY", "")
    â”‚
    â†“
Check: USE_PROXY = bool(PROXY_API_KEY)
    â”‚
    â”œâ”€ If True:  Initialize ProxyAPIClient() â†’ Uses provided API key
    â”‚
    â””â”€ If False: Use direct httpx scraping (Free)
```

## Proxy Service Support Matrix

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service      â”‚ API Key Type â”‚ Endpoint â”‚ Supports â”‚ Cost         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ScrapingBee  â”‚ api_key      â”‚ Standard â”‚ âœ… YES   â”‚ Free-$200/mo â”‚
â”‚ Bright Data  â”‚ user_key     â”‚ Custom   â”‚ âœ… YES   â”‚ $300+/mo     â”‚
â”‚ Apify        â”‚ api_key      â”‚ Standard â”‚ âœ… YES   â”‚ Free-$25+/mo â”‚
â”‚ Oxylabs      â”‚ user:pass    â”‚ Custom   â”‚ âœ… YES   â”‚ $200+/mo     â”‚
â”‚ SmartProxy   â”‚ api_key      â”‚ Standard â”‚ âœ… YES   â”‚ $0-300+/mo   â”‚
â”‚ Bright Data  â”‚ Standard     â”‚ HTTP API â”‚ âœ… YES   â”‚ Custom       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… Supported: Any service with standard HTTP API + API key authentication
âš ï¸ Requires: Adapter may need modification for custom endpoints
```

## Data Flow During Scraping

```
Scrape Job Created
    â”‚
    â”œâ”€ Create Job Record
    â”‚  â”œâ”€ job_id: UUID
    â”‚  â”œâ”€ keyword: "restaurant"
    â”‚  â”œâ”€ cities: ["Toledo, OH", "St Pete, FL"]
    â”‚  â””â”€ status: "running"
    â”‚
    â”œâ”€ For each city in parallel:
    â”‚  â”‚
    â”‚  â”œâ”€ For each page (1-25):
    â”‚  â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Fetch page
    â”‚  â”‚  â”‚  â”œâ”€ If USE_PROXY:
    â”‚  â”‚  â”‚  â”‚  â””â”€ ProxyAPIClient.fetch_url(url)
    â”‚  â”‚  â”‚  â””â”€ Else:
    â”‚  â”‚  â”‚     â””â”€ httpx.get(url)
    â”‚  â”‚  â”‚
    â”‚  â”‚  â”œâ”€ Parse HTML
    â”‚  â”‚  â”œâ”€ Extract listings (30 per page avg)
    â”‚  â”‚  â”‚
    â”‚  â”‚  â””â”€ For each listing:
    â”‚  â”‚     â”‚
    â”‚  â”‚     â”œâ”€ Fetch detail page
    â”‚  â”‚     â”œâ”€ Extract website
    â”‚  â”‚     â”œâ”€ Validate domain
    â”‚  â”‚     â”‚
    â”‚  â”‚     â”œâ”€ Check deduplication:
    â”‚  â”‚     â”‚  â”œâ”€ name + website + city = unique key
    â”‚  â”‚     â”‚  â””â”€ Skip if already in database
    â”‚  â”‚     â”‚
    â”‚  â”‚     â”œâ”€ Save to database (if new)
    â”‚  â”‚     â”‚
    â”‚  â”‚     â”œâ”€ Emit Redis event
    â”‚  â”‚     â”‚  â””â”€ job:{job_id}:events â†’ {type:"business"...}
    â”‚  â”‚     â”‚
    â”‚  â”‚     â””â”€ WebSocket broadcast to frontend
    â”‚  â”‚        â””â”€ Update results table
    â”‚  â”‚
    â”‚  â””â”€ After all pages for city:
    â”‚     â”œâ”€ Calculate extraction stats
    â”‚     â”œâ”€ Emit Redis metrics
    â”‚     â”‚  â””â”€ job:{job_id}:metrics â†’ {type:"extraction_stats"...}
    â”‚     â””â”€ Mark city task complete
    â”‚
    â””â”€ After all cities:
       â”œâ”€ Mark job complete
       â”œâ”€ Calculate overall statistics
       â””â”€ Enable CSV export
```

---

This architecture provides:
- âœ… **Flexibility**: Switch proxy providers without code changes
- âœ… **Scalability**: Multiple cities processed in parallel
- âœ… **Reliability**: Retry logic, error handling, deduplication
- âœ… **Real-time**: WebSocket updates to frontend during scraping
- âœ… **Cost efficiency**: Free direct scraping or any paid provider
- âœ… **Maintainability**: Generic, service-agnostic code
